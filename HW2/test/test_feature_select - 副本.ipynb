{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "house_df = pd.read_csv('../../data/properties_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete features with too many missing values\n",
    "nan = house_df.isnull().sum()\n",
    "nan = nan[nan < 2900000]\n",
    "nan_feature = nan.index.tolist()\n",
    "house_nan_df = house_df.loc[:,nan_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete categorical features\n",
    "cate_feature = ['airconditioningtypeid','fips','heatingorsystemtypeid','pooltypeid7','propertycountylandusecode','propertylandusetypeid',\n",
    "                'propertyzoningdesc','rawcensustractandblock','regionidcity','regionidcounty','regionidneighborhood','regionidzip',\n",
    "                'censustractandblock']\n",
    "\n",
    "for i in cate_feature:\n",
    "    nan_feature.remove(i)\n",
    "\n",
    "nan_cate_feature = nan_feature\n",
    "\n",
    "house_nan_cate_df = house_nan_df.loc[:,nan_cate_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete highly correlated features\n",
    "\n",
    "highcorr_feature = ['structuretaxvaluedollarcnt','threequarterbathnbr','finishedfloor1squarefeet','finishedsquarefeet12','finishedsquarefeet15', 'finishedsquarefeet50','taxvaluedollarcnt']\n",
    "\n",
    "for i in highcorr_feature:\n",
    "    nan_cate_feature.remove(i)\n",
    "\n",
    "nan_cate_co_feature = nan_cate_feature\n",
    "\n",
    "house_nan_co_cate_df = house_nan_cate_df.loc[:,nan_cate_co_feature]\n",
    "\n",
    "# rename the dataframe, after deleting many features\n",
    "house_new_df = house_nan_co_cate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2985217, 21)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_zero = 'poolcnt'\n",
    "house_new_df[nan_zero] = house_new_df[nan_zero].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_mean = ['parcelid', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid','calculatedbathnbr', 'calculatedfinishedsquarefeet', \n",
    "            'fireplacecnt', 'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude','lotsizesquarefeet',\n",
    "            'roomcnt', 'unitcnt', 'yearbuilt', 'numberofstories','assessmentyear','landtaxvaluedollarcnt', 'taxamount']\n",
    "\n",
    "house_new_df[nan_mean] = house_new_df[nan_mean].fillna(house_new_df[nan_mean].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "house_new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_norm = ['bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr','calculatedfinishedsquarefeet', 'fireplacecnt',\n",
    "                'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude','lotsizesquarefeet', 'poolcnt', 'roomcnt', \n",
    "                'unitcnt', 'yearbuilt', 'numberofstories','assessmentyear', 'landtaxvaluedollarcnt', 'taxamount']\n",
    "\n",
    "\n",
    "for col in cols_to_norm:\n",
    "    house_new_df[col] = (house_new_df[col] - house_new_df[col].mean())/(house_new_df[col].std())\n",
    "\n",
    "# house_new_df[cols_to_norm] = house_new_df[cols_to_norm].apply(lambda x: (x - x.mean()) / (x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../data/train_2016_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_df = train_df.merge(house_new_df, how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data set: prediction target\n",
    "\n",
    "target = join_df.loc[:,'logerror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data set: attributes\n",
    "\n",
    "train = join_df.iloc[:,2:]\n",
    "\n",
    "# feature = join_df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert datetime into float\n",
    "\n",
    "for num,str in enumerate(train.transactiondate):\n",
    "        date_time = datetime.datetime.strptime(str,'%Y-%m-%d')\n",
    "        str = time.mktime(date_time.timetuple())\n",
    "        train.set_value(num,'transactiondate',str)\n",
    "        \n",
    "train['transactiondate'] = train['transactiondate'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def z_score_norm(df, feature_list):\n",
    "    \"\"\" Dot the z-score method on selected features in feature_list. \n",
    "        z = (x - mean)/std\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): data set\n",
    "        feature_list   (list): a list of features, each element is a string\n",
    "    \"\"\"\n",
    "    \n",
    "    for col in feature_list:\n",
    "        df[col] = (df[col] - df[col].mean())/(df[col].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_norm(train, ['transactiondate'])\n",
    "train['transactiondate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = RandomForestRegressor(max_depth=6, random_state=0)\\nmodel = model.fit(feature, target)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models\n",
    "\n",
    "\n",
    "'''\n",
    "model = linear_model.LinearRegression()\n",
    "model = model.fit(feature, target)\n",
    "'''\n",
    "\n",
    "\n",
    "model = MLPRegressor()\n",
    "model = model.fit(train, target)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "model = linear_model.Ridge(alpha=1.0)\n",
    "model = model.fit(train, target)\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "model = RandomForestRegressor(max_depth=6, random_state=0)\n",
    "model = model.fit(feature, target)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"../../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1 = sample.ParcelId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1.columns = ['parcelid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1 = sample1.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1 = sample.ParcelId\n",
    "sample1 = sample1.to_frame()\n",
    "sample1.columns = ['parcelid']\n",
    "sample1.describe()\n",
    "df_test = sample1.merge(house_new_df, on='parcelid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['parcelid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_transform(df_test):\n",
    "    for num,str in enumerate(df_test.transactiondate):\n",
    "        date_time = datetime.datetime.strptime(str,'%Y-%m-%d')\n",
    "        str = time.mktime(date_time.timetuple())\n",
    "        df_test.set_value(num,'transactiondate',str)\n",
    "    df_test['transactiondate'] = df_test['transactiondate'].astype(float) \n",
    "    z_score_norm(df_test, ['transactiondate'])\n",
    "    test_matrix = df_test.as_matrix()\n",
    "    return test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-9224ae7dc709>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf_test1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'transactiondate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2016-10-15'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdf_test1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mpred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'201610'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpred1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \"\"\"\n\u001b[0;32m   1256\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coefs_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \"\"\"\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;31m# Make sure self.hidden_layer_sizes is a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 58\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "result = sample1.copy()\n",
    "result.columns = ['ParcelId']\n",
    "\n",
    "\n",
    "df_test1 = df_test.copy()\n",
    "df_test1.insert(0,'transactiondate','2016-10-15')\n",
    "df_test1 = date_transform(df_test1)\n",
    "pred1 = model.predict(df_test1)\n",
    "pred1 = np.asarray(pred1)\n",
    "result.insert(1,'201610',pred1)\n",
    "del df_test1\n",
    "\n",
    "\n",
    "df_test2 = df_test.copy()\n",
    "df_test2.insert(0,'transactiondate','2016-11-15')\n",
    "df_test2 = date_transform(df_test2)\n",
    "pred2 = model.predict(df_test2)\n",
    "pred2 = np.asarray(pred2)\n",
    "result.insert(2,'201611',pred2)\n",
    "del df_test2\n",
    "\n",
    "\n",
    "df_test3 = df_test.copy()\n",
    "df_test3.insert(0,'transactiondate','2016-12-15')\n",
    "df_test3 = date_transform(df_test3)\n",
    "pred3 = model.predict(df_test3)\n",
    "pred3 = np.asarray(pred3)\n",
    "result.insert(3,'201612',pred3)\n",
    "del df_test3\n",
    "\n",
    "\n",
    "df_test4 = df_test.copy()\n",
    "df_test4.insert(0,'transactiondate','2017-10-15')\n",
    "df_test4 = date_transform(df_test4)\n",
    "pred4 = model.predict(df_test4)\n",
    "pred4 = np.asarray(pred4)\n",
    "result.insert(4,'201710',pred4)\n",
    "del df_test4\n",
    "\n",
    "\n",
    "df_test5 = df_test.copy()\n",
    "df_test5.insert(0,'transactiondate','2017-11-15')\n",
    "df_test5 = date_transform(df_test5)\n",
    "pred5 = model.predict(df_test5)\n",
    "pred5 = np.asarray(pred5)\n",
    "result.insert(5,'201711',pred5)\n",
    "del df_test5\n",
    "\n",
    "\n",
    "df_test6 = df_test.copy()\n",
    "df_test6.insert(0,'transactiondate','2017-12-15')\n",
    "df_test6 = date_transform(df_test6)\n",
    "pred6 = model.predict(df_test6)\n",
    "pred6 = np.asarray(pred6)\n",
    "result.insert(6,'201712',pred6)\n",
    "del df_test6\n",
    "\n",
    "# result.columns = ['ParcelId', '201610']\n",
    "\n",
    "result.columns = ['ParcelId', '201610', '201611', '201612', '201710','201711','201712']\n",
    "result.to_csv('../predictions/sample12.csv',index=False,header=True)  # need to change filename per run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
