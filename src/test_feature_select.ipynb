{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df = pd.read_csv('../data/properties_2016_backup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete features with too many missing values\n",
    "nan = house_df.isnull().sum()\n",
    "nan = nan[nan < 2900000]\n",
    "nan_feature = nan.index.tolist()\n",
    "house_nan_df = house_df.loc[:,nan_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete categorical features\n",
    "cate_feature = ['airconditioningtypeid','fips','heatingorsystemtypeid','pooltypeid7','propertycountylandusecode','propertylandusetypeid',\n",
    "                'propertyzoningdesc','rawcensustractandblock','regionidcity','regionidcounty','regionidneighborhood','regionidzip',\n",
    "                'censustractandblock']\n",
    "\n",
    "for i in cate_feature:\n",
    "    nan_feature.remove(i)\n",
    "\n",
    "nan_cate_feature = nan_feature\n",
    "\n",
    "house_nan_cate_df = house_nan_df.loc[:,nan_cate_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete highly correlated features\n",
    "\n",
    "highcorr_feature = ['finishedsquarefeet12','finishedsquarefeet15', 'finishedsquarefeet50','taxvaluedollarcnt']\n",
    "\n",
    "for i in highcorr_feature:\n",
    "    nan_cate_feature.remove(i)\n",
    "\n",
    "nan_cate_co_feature = nan_cate_feature\n",
    "\n",
    "house_nan_co_cate_df = house_nan_cate_df.loc[:,nan_cate_co_feature]\n",
    "\n",
    "# rename the dataframe, after deleting many features\n",
    "house_new_df = house_nan_co_cate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_zero = 'poolcnt'\n",
    "house_new_df[nan_zero] = house_new_df[nan_zero].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_new_df['poolcnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_mean = ['parcelid', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid','calculatedbathnbr', 'finishedfloor1squarefeet',\n",
    "       'calculatedfinishedsquarefeet', 'fireplacecnt', 'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude',\n",
    "       'lotsizesquarefeet', 'roomcnt', 'threequarterbathnbr', 'unitcnt', 'yearbuilt', 'numberofstories', 'structuretaxvaluedollarcnt',\n",
    "       'assessmentyear', 'landtaxvaluedollarcnt', 'taxamount']\n",
    "\n",
    "house_new_df[nan_mean] = house_new_df[nan_mean].fillna(house_new_df[nan_mean].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "house_new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_norm = ['bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr', 'finishedfloor1squarefeet',\n",
    "                'calculatedfinishedsquarefeet', 'fireplacecnt', 'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude',\n",
    "                'lotsizesquarefeet', 'poolcnt', 'roomcnt', 'threequarterbathnbr', 'unitcnt', 'yearbuilt', 'numberofstories', \n",
    "                'structuretaxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt', 'taxamount']\n",
    "\n",
    "\n",
    "for col in cols_to_norm:\n",
    "    house_new_df[col] = (house_new_df[col] - house_new_df[col].mean())/(house_new_df[col].std())\n",
    "\n",
    "# house_new_df[cols_to_norm] = house_new_df[cols_to_norm].apply(lambda x: (x - x.mean()) / (x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_new_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_2016_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_df = train_df.merge(house_new_df, how='left', on='parcelid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data set: prediction target\n",
    "\n",
    "target = join_df.loc[:,'logerror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data set: attributes\n",
    "\n",
    "train = join_df.iloc[:,2:]\n",
    "\n",
    "# feature = join_df.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert datetime into float\n",
    "\n",
    "for num,str in enumerate(train.transactiondate):\n",
    "        date_time = datetime.datetime.strptime(str,'%Y-%m-%d')\n",
    "        str = time.mktime(date_time.timetuple())\n",
    "        train.set_value(num,'transactiondate',str)\n",
    "        \n",
    "train['transactiondate'] = train['transactiondate'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "lr = lr.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "\n",
    "'''\n",
    "model = linear_model.LinearRegression()\n",
    "model = model.fit(feature, target)\n",
    "'''\n",
    "\n",
    "'''\n",
    "model = linear_model.Ridge(alpha=1.0)\n",
    "model = model.fit(feature, target)\n",
    "'''\n",
    "\n",
    "model = RandomForestRegressor(max_depth=6, random_state=0)\n",
    "model = model.fit(feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"../data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = sample.ParcelId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1.columns = ['parcelid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = sample1.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample1 = sample.ParcelId\n",
    "sample1 = sample1.to_frame()\n",
    "sample1.columns = ['parcelid']\n",
    "sample1.describe()\n",
    "df_test = sample1.merge(house_new_df, on='parcelid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_test['parcelid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_transform(df_test):\n",
    "    for num,str in enumerate(df_test.transactiondate):\n",
    "        date_time = datetime.datetime.strptime(str,'%Y-%m-%d')\n",
    "        str = time.mktime(date_time.timetuple())\n",
    "        df_test.set_value(num,'transactiondate',str)\n",
    "    df_test['transactiondate'] = df_test['transactiondate'].astype(float) \n",
    "    \n",
    "    test_matrix = df_test.as_matrix()\n",
    "    return test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sample1.copy()\n",
    "result.columns = ['ParcelId']\n",
    "\n",
    "\n",
    "df_test1 = df_test.copy()\n",
    "df_test1.insert(0,'transactiondate','2016-10-15')\n",
    "df_test1 = date_transform(df_test1)\n",
    "pred1 = model.predict(df_test1)\n",
    "pred1 = np.asarray(pred1)\n",
    "result.insert(1,'201610',pred1)\n",
    "del df_test1\n",
    "\n",
    "\n",
    "df_test2 = df_test.copy()\n",
    "df_test2.insert(0,'transactiondate','2016-11-15')\n",
    "df_test2 = date_transform(df_test2)\n",
    "pred2 = model.predict(df_test2)\n",
    "pred2 = np.asarray(pred2)\n",
    "result.insert(2,'201611',pred2)\n",
    "del df_test2\n",
    "\n",
    "\n",
    "df_test3 = df_test.copy()\n",
    "df_test3.insert(0,'transactiondate','2016-12-15')\n",
    "df_test3 = date_transform(df_test3)\n",
    "pred3 = model.predict(df_test3)\n",
    "pred3 = np.asarray(pred3)\n",
    "result.insert(3,'201612',pred3)\n",
    "del df_test3\n",
    "\n",
    "\n",
    "df_test4 = df_test.copy()\n",
    "df_test4.insert(0,'transactiondate','2017-10-15')\n",
    "df_test4 = date_transform(df_test4)\n",
    "pred4 = model.predict(df_test4)\n",
    "pred4 = np.asarray(pred4)\n",
    "result.insert(4,'201710',pred4)\n",
    "del df_test4\n",
    "\n",
    "\n",
    "df_test5 = df_test.copy()\n",
    "df_test5.insert(0,'transactiondate','2017-11-15')\n",
    "df_test5 = date_transform(df_test5)\n",
    "pred5 = model.predict(df_test5)\n",
    "pred5 = np.asarray(pred5)\n",
    "result.insert(5,'201711',pred5)\n",
    "del df_test5\n",
    "\n",
    "\n",
    "df_test6 = df_test.copy()\n",
    "df_test6.insert(0,'transactiondate','2017-12-15')\n",
    "df_test6 = date_transform(df_test6)\n",
    "pred6 = model.predict(df_test6)\n",
    "pred6 = np.asarray(pred6)\n",
    "result.insert(6,'201712',pred6)\n",
    "del df_test6\n",
    "\n",
    "# result.columns = ['ParcelId', '201610']\n",
    "\n",
    "result.columns = ['ParcelId', '201610', '201611', '201612', '201710','201711','201712']\n",
    "result.to_csv('../predictions/sample4.csv',index=False,header=True)  # need to change filename per run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
